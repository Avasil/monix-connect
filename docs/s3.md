---
id: s3
title: AWS S3
---

## Introduction

The object storage service that offers industry leading scalability, availability, security and performance.
It allows data storage of any amount of data, commonly used as a data lake for big data applications which can now be easily integrated with monix.
 
 The module has been implemented using the `S3AsyncClient` since it only exposes non blocking methods. 
 Therefore, all of the monix s3 methods defined in the `S3` object would expect an implicit instance of 
 this class to be in the scope of the call.
   
## Dependency
 
 Add the following dependency:
 
 ```scala
 libraryDependencies += "io.monix" %% "monix-s3" % "0.1.0"
 ```

## Getting started 

 First thing is to create the s3 client that will allow us to authenticate and create an channel between our 
 application and the AWS S3 service. 
 
 ### Connection
 So the below code shows an example on how to set up this connection. Note that in this case 
  the authentication is done thorugh AWS S3 using access and secret keys, 
  but you might use another method such as IAM roles.
 
 ```scala
import java.net.URI
import software.amazon.awssdk.auth.credentials.{AwsBasicCredentials, StaticCredentialsProvider}
import software.amazon.awssdk.services.s3.S3AsyncClient
import software.amazon.awssdk.regions.Region.AWS_GLOBAL

val basicAWSCredentials: AwsBasicCredentials = AwsBasicCredentials.create(s3AccessKey, s3SecretKey)
val credentialsProvider: StaticCredentialsProvider = StaticCredentialsProvider.create(basicAWSCredentials)

// Note that the client is defined as implicit, this is on purpose since each of the methods defined in
// the monix s3 connector will expect that.
 implicit val s3Client: S3AsyncClient = S3AsyncClient
    .builder()
    .credentialsProvider(credentialsProvider)
    .region(AWS_GLOBAL)
    .endpointOverride(URI.create(endPoint))//this one is used to point to the localhost s3 service, not used in prod 
    .build
```

  ### Create and delete 

Once we have configured the s3 client, let's start with the basic operations to _create_ and _delete_ buckets:
 ```scala
import software.amazon.awssdk.services.s3.model.{CreateBucketResponse, DeleteBucketResponse}

val bucketName: String = "myBucket" 
val _: Task[CreateBucketResponse] = S3.createBucket(bucketName)
val _: Task[DeleteBucketResponse] = S3.deleteBucket(bucketName)
```

  ### Delete object
You can also operate at object level within a bucket with:
 ```scala
import software.amazon.awssdk.services.s3.model.{DeleteObjectResponse, ListObjectsResponse}

val bucketName: String = "myBucket" 
val _: Task[DeleteObjectResponse] = S3.deleteObject(bucketName)
val _: Task[ListObjectsResponse] = S3.listObjects(bucketName)
```

  ### List objects

 ```scala
import software.amazon.awssdk.services.s3.model.{DeleteObjectResponse, ListObjectsResponse}

val bucketName: String = "myBucket" 
val _: Task[DeleteObjectResponse] = S3.deleteObject(bucketName)
val _: Task[ListObjectsResponse] = S3.listObjects(bucketName)
```

  ### Get object

 ```scala
val objectKey: String = "/object/file.txt"
val _: Task[Array[Byte]] = S3.getObject(bucketName, objectKey)
}
```

  ### Put object

On the other hand, to put objects:
 ```scala
import software.amazon.awssdk.services.s3.model.PutObjectResponse

val content: Array[Byte] = "file content".getBytes()
val _: Task[PutObjectResponse] = S3.putObject(bucketName, objectKey, content)
}
```

### Multipart update

Finally, for dealing with large files of data you might want to use the `multipartUpload` consumer.
This one consumes an observable and synchronously makes partial uploads of the incoming chunks. 

Thus, it reduces substantially the risk on having jvm overhead errors or getting http requests failures, 
since the whole file does not need to be allocated in the memory and the http request body won't be that big. 

The partial uploads can be fine tuned by the minimum chunksize that will be sent, being 5MB the default minimum size (equally as an integer value of 5242880).

```scala
import software.amazon.awssdk.services.s3.model.CompleteMultipartUploadResponse

// given an strem of chunks (Array[Byte]) 
val ob: Observable[Array[Byte]] = Observable.fromIterable(chunks)

// and a multipart upload consumer
val multipartUploadConsumer: Consumer[Array[Byte], Task[CompleteMultipartUploadResponse]] =
  S3.multipartUpload(bucketName, objectKey)

// then
ob.fromIterable(chunks).consumeWith(multipartUploadConsumer)
```

## Local testing

For AWS S3 local testing we went with [minio](https://github.com/minio/minio) instead of localstack, since we found an [issue](https://github.com/localstack/localstack/issues/538) that can block you on writing your functional tests.

Add the following service description to your `docker-compose.yaml` file:

```yaml
minio:
  image: minio/minio
  ports:
    - "9000:9000"
  volumes:
    - ./minio/data:/data
  environment:
    - MINIO_ACCESS_KEY=TESTKEY
    - MINIO_SECRET_KEY=TESTSECRET
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
    interval: 35s
    timeout: 20s
    retries: 3
  command: server --compat /data
```

Run the following command to build, and start the redis server:

```shell script
docker-compose -f docker-compose.yml up -d minio
``` 
Check out that the service has started correctly, notice that a healthcheck has been defined on the description of the minio service, 
that's because minio s3 is a very heavy image and sometimes it takes too long to be set up or sometime it even fails, so that would prevent those cases.

Finally, create the connection with AWS S3, note that minio does not has support for `AnonymousCredentialsProvider`, 
therefore you'll have to use `AwsBasicCredentials`, in which the _key_ and _secret_ will correspond respectively to the
 defined environment variables from docker compose definition `MINIO_ACCESS_KEY` and `MINIO_SECRET_KEY`. 

```scala
import software.amazon.awssdk.regions.Region.AWS_GLOBAL
import software.amazon.awssdk.services.s3.S3AsyncClient
import software.amazon.awssdk.auth.credentials.{AwsBasicCredentials, StaticCredentialsProvider}

val minioEndPoint: String = "http://localhost:9000"

val s3AccessKey: String = "TESTKEY" //see docker minio env var `MINIO_ACCESS_KEY`
val s3SecretKey: String = "TESTSECRET" //see docker minio env var `MINIO_SECRET_KEY`

val basicAWSCredentials = AwsBasicCredentials.create(s3AccessKey, s3SecretKey)
implicit val s3AsyncClient: S3AsyncClient = S3AsyncClient
  .builder()
  .credentialsProvider(StaticCredentialsProvider.create(basicAWSCredentials))
  .region(AWS_GLOBAL)
  .endpointOverride(URI.create(minioEndPoint))
  .build
``` 

Now you are ready to run your application! 

_Note that the above example defines the client as `implicit`, since it is how the api will expect this one._

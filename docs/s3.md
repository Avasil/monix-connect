---
id: s3
title: AWS S3
---

## Introduction

_AWS Simple Storage Service_ ([S3](https://aws.amazon.com/s3/?nc=sn&loc=0)) is an object storage service that offers industry leading scalability, availability, security and performance.
It allows data storage of any amount of data, commonly used as a data lake for big data applications which can now be easily integrated with monix.
   
## Dependency
 
 Add the following dependency:
 
 ```scala
 libraryDependencies += "io.monix" %% "monix-s3" % "0.3.3"
 ```
 
## AsyncClient
 
 First of all, we need to create the _S3_ client that will allow us to authenticate and create an non blocking channel between our 
 application and the AWS service. 
 
 This connector uses the _underlying_ `S3AsyncClient` from the [java aws sdk](https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/services/s3/model/package-summary.html),
 since it only exposes non blocking methods. 
 This, the api is exposed under the `monix.connect.s3.S3` object and they all expects an _implicit_ instance of 
 this async client class to be in the scope of the call.
 
 Below code shows is just an example on how to set up this connection to the AWS S3, you must configure properly to your use case. 
 See in how to use it locally in [the last section](##Local testing) of this page.
 Note that in this case the authentication is using AWS access and secret keys, but you might use another method such an _IAM_ role.
 
 ```scala
import java.time.Duration

import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider
import software.amazon.awssdk.services.s3.S3AsyncClient
import software.amazon.awssdk.regions.Region.AWS_GLOBAL
import software.amazon.awssdk.http.nio.netty.NettyNioAsyncHttpClient

/* exceptions related with concurrency or timeouts from any of the requests
 * might be solved by raising the maxConcurrency, maxPendingConnectionAcquire or
 * connectionAcquisitionTimeout from the underlying netty http client.
 * see in below code how do so, the values are just example ones.
 */
val httpClient = NettyNioAsyncHttpClient.builder()
  .maxConcurrency(500)
  .maxPendingConnectionAcquires(50000)
  .connectionAcquisitionTimeout(Duration.ofSeconds(60))
  .readTimeout(Duration.ofSeconds(60))
  .build()

// note that the client is defined as implicit, and this is on purpose since 
// each of the methods will expect it in that way.
implicit val s3AsyncClient: S3AsyncClient = S3AsyncClient
  .builder()
  .httpClient(httpClient) //optional and needs to be properly configured
  .credentialsProvider(DefaultCredentialsProvider.create())
  .region(AWS_GLOBAL)
  .build
```


### Create bucket 

Once you have configured the client, you would probably need to _create a bucket_:

 ```scala
import software.amazon.awssdk.services.s3.model.CreateBucketResponse
import monix.connect.s3.S3

val bucketName: String = "my-bucket" 
val t: Task[CreateBucketResponse] = S3.createBucket(bucketName)
```
## Delete bucket 

 On the other hand if you want to remove the bucket:
 
 ```scala
import software.amazon.awssdk.services.s3.model.DeleteBucketResponse
import monix.connect.s3.S3

val bucketName: String = "my-bucket" 
val t: Task[DeleteBucketResponse] = S3.deleteBucket(bucketName)
```

## Delete object
  
You can also delete an _object_ within the specified _bucket_ with:
 ```scala
import software.amazon.awssdk.services.s3.model.{DeleteObjectResponse, ListObjectsResponse}
import monix.connect.s3.S3

val bucketName: String = "my-bucket" 
val key: String = "path/to/my/object.txt" 

val t: Task[DeleteObjectResponse] = S3.deleteObject(bucketName, key)
```

## Exists bucket
  
Check whether the specified bucket exists or not.

 ```scala
import monix.connect.s3.S3

val bucketName: String = "my-bucket" 

val t: Task[Boolean] = S3.existsBucket(bucketName)
```


## Exists object
  
Check whether the specified object exists or not.

 ```scala
import monix.connect.s3.S3

val bucketName: String = "my-bucket" 
val key: String = "path/to/my/object.txt" 


val t: Task[Boolean] = S3.existsObject(bucketName, key)
```

## List objects
  
Lists all the objects within a bucket and prefix:

 ```scala
import software.amazon.awssdk.services.s3.model.{DeleteObjectResponse, ListObjectsResponse}
import software.amazon.awssdk.services.s3.model.S3Object
import monix.connect.s3.S3
import monix.reactive.Observable

val bucketName = "my-bucket"
val prefix = s"prefix/to/list/keys/"

val s3Objects: Observable[S3Object] = S3.listObjects(bucketName, prefix = Some(prefix))

// use `maxTotalKeys` to set a limit to the number of keys you want to fetch
val s3ObjectsWithLimit: Observable[S3Object] = 
  S3.listObjects(bucketName, maxTotalKeys = Some(1011), prefix = Some(prefix))
```

Listing a very long list of objects might take time and and cause the underlying netty connection to error because 
of the request timeout, in order increase that see fix that you can increase those timeouts with a new configuration for `software.amazon.awssdk.http.nio.netty.NettyNioAsyncHttpClient`
that will be added to the `software.amazon.awssdk.services.s3.S3AsyncClient`.


### Download
  
Download the specified object as a single byte array. 
Note that this operation is only suitable for small objects, 
since it dangerous to perform on large objects `downloadMultipart` should be used.

 ```scala
import software.amazon.awssdk.services.s3.model.PutObjectResponse
import monix.connect.s3.S3
import monix.eval.Task

val bucketName = "my-bucket"
val key = "sample/download/path/file.txt"
val content: Array[Byte] = "Dummy file content".getBytes()

val t: Task[Array[Byte]] = S3.download("my-bucket", key)
```

### Download multipart
  
A method that _safely_ downloads objects of any size by performing partial download requests.
The number of bytes to get per each request is specified by the [[chunkSize]].

 ```scala
import monix.connect.s3.S3
import monix.reactive.Observable

val bucketName = "my-bucket"
val key = "sample/path/file.json"

val ob: Observable[Array[Byte]] = S3.downloadMultipart("my-bucket", key)
```

## Upload

You can also easily upload an object into an _S3_  with the _upload_ signature.
Note that if you need to update large amount of data you should not be using this method, see instead [multipartUpload](###Multipart)`.

 ```scala
import software.amazon.awssdk.services.s3.model.PutObjectResponse
import monix.connect.s3.S3
import monix.eval.Task

val bucketName = "my-bucket"
val key = "sample/upload/path/file.txt"
val content: Array[Byte] = "Dummy file content".getBytes()

val t: Task[PutObjectResponse] = S3.upload("my-bucket", key, content)
```

## Multipart upload

When dealing with large files of data you should use the `multipartUpload` operation.
This one can be used to consume an observable of bytes while sending partial upload request, if chunk was smaller than the minimum size it will be concatenated with the next request. 

Thus, reducing substantially the risk on having _OOM_ errors or _failed http requests_ due to the size of the body.

But a very important one to have present is by the minimum `chunksize` that will be sent, being _5MB_ or _5242880 bytes_ the _default_ and _minimum_ allowed size (lower values would result in failure).

Note that the method can be tuned with specific _aws configurations_ such as `acl`, `requestPayer`, etc. 

```scala
import software.amazon.awssdk.services.s3.model.CompleteMultipartUploadResponse
import monix.connect.s3.S3
import monix.reactive.Observable
import monix.reactive.Consumer
import monix.eval.Task

val bucketName = "my-bucket"
val key = "sample/upload/path/file.txt"

// preasumably we have a stream of bytes coming in
val ob = Observable.fromIterable(List("A", "B", "C").map(_.getBytes))

// and a multipart upload consumer
val uploadConsumer: Consumer[Array[Byte], Task[CompleteMultipartUploadResponse]] =
  S3.multipartUpload(bucketName, key)

// then
val r: Task[CompleteMultipartUploadResponse] = ob.consumeWith(uploadConsumer)
```


## Local testing

There is actually a very good support on regards testing `AWS S3`, you could either spin up a docker image to just use a JVM library that emulates such service.

The following sections describes different alternatives:
 
### Localstack

 A fully functional local _AWS_ cloud stack available as a docker image.
 
 You would just need to define it as a service in your `docker-compose.yml`:
 
 ```yaml
 localstack:
    image: localstack/localstack:latest
    hostname: localstack
    container_name: localstack
    ports:
      - '4566:4566'
    environment:
      - SERVICES=s3
# very important to specify `s3` on `SERVICES` env var, it would prevent to spin up the rest of the AWS services.
``` 

 Then, run the following command to build and start the _S3_ service:
 
 ```shell script
 docker-compose -f ./docker-compose.yml up -d localstack
```

A good point on favor to using _localstack_ is that it provides support for _AWS Anonymous Credentials_, meaning that you can easily connect to your 
local S3 service with no required authentication. 
 
See below an example on how create the async client:
 
 ```scala
import software.amazon.awssdk.regions.Region.AWS_GLOBAL
import software.amazon.awssdk.services.s3.S3AsyncClient
import software.amazon.awssdk.auth.credentials.AnonymousCredentialsProvider
import java.net.URI

val localStackEndpoint = "http://localhost:4566"
implicit val s3AsyncClient: S3AsyncClient = S3AsyncClient
  .builder()
  .credentialsProvider(new AnonymousCredentialsProvider)
  .region(AWS_GLOBAL)
  .endpointOverride(URI.create(localStackEndpoint))
  .build
 ```
 _Notice_ that the the _client_ was defined as `implicit` since it is how the api will expect it.
 
**Important** - Whenever you create a bucket on localstack, you would better set its _ACL (Access Control List)_ as `public-read` since it might prevent you to encounter [403 access denied](localstack/localstack#406) when reading.
If you set the `container_name` field to _localstack_ in `docker-compose.yaml` you can create the bucket and specify the right _ACL_ like:
```shell script
docker exec localstack awslocal s3 mb s3://my-bucket
docker exec localstack awslocal s3api put-bucket-acl --bucket my-bucket --acl public-read
``` 
On the other hand, if prefer to do that from code:

```scala
import monix.connect.s3.S3
import org.scalatest.BeforeAndAfterAll

override def beforeAll() = {
  super.beforeAll()
  S3.createBucket("my-bucket", acl = Some("public-read")).runSyncUnsafe()
}
```

### Minio
 
[Minio](https://github.com/minio/minio) is another well known docker image that emulates _AWS S3_.

The advantages of using _minio_ over _localstack_ is that it provides a beautiful _UI_ that allows you to quickly visualize and manage the 
 objects and buckets stored in the local S3. 
 On the other hand, a disadvantage could be that it does not support _Anonymous Credentials_, so you have to specify _key_ and _secret_ to create the connection.
 
The following service description to your `docker-compose.yaml` file:

```yaml
minio:
  image: minio/minio
  ports:
    - "9000:9000"
  volumes:
    - ./minio/data:/data
  environment:
    - MINIO_ACCESS_KEY=TESTKEY
    - MINIO_SECRET_KEY=TESTSECRET
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
    interval: 35s
    timeout: 20s
    retries: 3
  command: server --compat /data
```

Then, run the following command to build and start the S3 service:

```shell script
docker-compose -f ./docker-compose.yml up -d minio
``` 

Check out that the service has started correctly, notice that a _healthcheck_ has been defined on the description of the minio service, 
that's because it is a heavy image and sometimes it takes bit long to start or it even fails, so adding it will prevent that to happen.

Finally you can already create the connection to _AWS S3_, _notice_ that _minio_ does not support _Anonymous credentials_, instead you'll have to use _Basic Credentials_ and specify the _key_ and _secret_ corresponding respectively to the
 defined environment variables `MINIO_ACCESS_KEY` and `MINIO_SECRET_KEY`.

```scala
import software.amazon.awssdk.regions.Region.AWS_GLOBAL
import software.amazon.awssdk.services.s3.S3AsyncClient
import software.amazon.awssdk.auth.credentials.{AwsBasicCredentials, StaticCredentialsProvider}
import java.net.URI

val minioEndPoint: String = "http://localhost:9000"

val s3AccessKey: String = "TESTKEY" //equal to the env var `MINIO_ACCESS_KEY`  
val s3SecretKey: String = "TESTSECRET" //equal to the `env var `MINIO_SECRET_KEY`

val basicAWSCredentials = AwsBasicCredentials.create(s3AccessKey, s3SecretKey)
implicit val s3AsyncClient: S3AsyncClient = S3AsyncClient
  .builder()
  .credentialsProvider(StaticCredentialsProvider.create(basicAWSCredentials))
  .region(AWS_GLOBAL)
  .endpointOverride(URI.create(minioEndPoint))
  .build
```

### JVM S3 Mock library

In case you prefer to _start_ and _stop_ the _S3_ service from from the code of same test and therefore not depending on _docker_ but just on a _JVM library dependency_, you can refer to [findify/s3Mock](https://github.com/findify/s3mock) to see more. 
